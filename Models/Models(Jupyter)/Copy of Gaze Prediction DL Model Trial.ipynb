{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Gaze Prediction DL Model Trial.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HuAXjDGJPDkw","executionInfo":{"status":"ok","timestamp":1658425372796,"user_tz":240,"elapsed":31172,"user":{"displayName":"Shunji Wan","userId":"11173786701409222850"}},"outputId":"5a6cb574-8282-42bd-aae6-c97e5b824e82"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mediapipe\n","  Downloading mediapipe-0.8.10.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.9 MB)\n","\u001b[K     |████████████████████████████████| 32.9 MB 1.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.21.6)\n","Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.17.3)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (21.4.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.2.2)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.6.0.66)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.2.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<4,>=3.11->mediapipe) (1.15.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (1.4.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (3.0.9)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mediapipe) (4.1.1)\n","Installing collected packages: mediapipe\n","Successfully installed mediapipe-0.8.10.1\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install mediapipe\n","import mediapipe as mp\n","import os\n","import json"]},{"cell_type":"markdown","source":["\n","\n","# Load Trained VAE Model"],"metadata":{"id":"qT7A8KJyPdNX"}},{"cell_type":"code","source":["vae_path = '/content/drive/Shareddrives/URSI 2022/Eye Tracking ML/vae_encoder/vae_2022-07-20_15:24:39'\n","vae_encoder = tf.keras.models.load_model(vae_path)\n","\n","vae_encoder.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sXwLgQS4Pkbq","executionInfo":{"status":"ok","timestamp":1658426322559,"user_tz":240,"elapsed":5773,"user":{"displayName":"Shunji Wan","userId":"11173786701409222850"}},"outputId":"47965937-470e-4250-ef83-af932bdae8b2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n","Model: \"model_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," vae_encoder_input (InputLay  [(None, 68, 3)]          0         \n"," er)                                                             \n","                                                                 \n"," vae_flatten (Flatten)       (None, 204)               0         \n","                                                                 \n"," vae_dense_1 (Dense)         (None, 200)               41000     \n","                                                                 \n"," vae_dense_2 (Dense)         (None, 100)               20100     \n","                                                                 \n"," vae_dense_3 (Dense)         (None, 50)                5050      \n","                                                                 \n"," z_mean (Dense)              (None, 6)                 306       \n","                                                                 \n","=================================================================\n","Total params: 66,456\n","Trainable params: 66,456\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["# Load MediaPipe model to get the set of mesh points"],"metadata":{"id":"EgXaAHvTPrEg"}},{"cell_type":"code","source":["mp_face_mesh = mp.solutions.face_mesh\n","\n","left_eye_point = set(sum(mp_face_mesh.FACEMESH_LEFT_EYE, ()))\n","right_eye_point = set(sum(mp_face_mesh.FACEMESH_RIGHT_EYE, ()))\n","left_iris_point = set(sum(mp_face_mesh.FACEMESH_LEFT_IRIS, ()))\n","right_iris_point = set(sum(mp_face_mesh.FACEMESH_RIGHT_IRIS, ()))\n","\n","face_oval_point = set(sum(mp_face_mesh.FACEMESH_FACE_OVAL, ()))\n","\n","#keypoints = left_eye_point.union(right_eye_point).union(left_iris_point).union(right_iris_point)\n","\n","keypoints = left_eye_point.union(right_eye_point).union(face_oval_point)\n","\n","keypoints = sorted(list(keypoints))\n"],"metadata":{"id":"QnZt2oFiPtaq","executionInfo":{"status":"ok","timestamp":1658426325773,"user_tz":240,"elapsed":142,"user":{"displayName":"Shunji Wan","userId":"11173786701409222850"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Load in one JSON file as an example"],"metadata":{"id":"P2M5S4HsPwJ4"}},{"cell_type":"code","source":["json_path = '/content/drive/Shareddrives/URSI 2022/Eye Tracking ML/json/'\n","all_json_files = os.listdir(json_path)\n","\n","with open(json_path + 'fwkruums.json', 'r') as file:\n","    json_data = json.load(file)"],"metadata":{"id":"rbdov9QlSVys","executionInfo":{"status":"ok","timestamp":1658426338775,"user_tz":240,"elapsed":8101,"user":{"displayName":"Shunji Wan","userId":"11173786701409222850"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Helper Functions\n"],"metadata":{"id":"MeWgu09-4nGa"}},{"cell_type":"code","source":["import statistics\n","import math\n","\n","# Return coefficients a, b that represent the straight line \n","# constructed by the given points pt1, pt2 (y = ax + b)\n","def get_line(pt1, pt2):\n","  x1, y1 = pt1\n","  x2, y2 = pt2\n","  a = (y2 - y1) / (x2 - x1)\n","  b = y1 - (a * x1)\n","  return [a, b]\n","\n","# Return the coordinate of intersection of two straight lines\n","# l1, l2 in terms of [x, y]\n","def get_intersection(l1, l2):\n","  a1, b1 = l1\n","  a2, b2 = l2\n","  x = (b2 - b1) / (a1 - a2)\n","  y = a1 * x + b1\n","  return [x, y]\n","\n","# Return the angle that a vector needs to rotate counter-clockwisely\n","# in order to point at the same direction as the x-axis\n","def get_ccw_angle(vector):\n","  x, y = vector\n","  tan = y / x\n","  r = math.atan(tan)\n","  if x >= 0 and y > 0:\n","    pass\n","  elif x < 0 and y >= 0:\n","    r = r + math.pi\n","  elif x <= 0 and y < 0:\n","    r = r + math.pi\n","  elif x > 0 and y <= 0:\n","    r = r + 2 * math.pi\n","  else:\n","    r = 0\n","  return r\n","\n","# Given a list of 4 landmarks on the face mesh, construct a new coordinate\n","# system relative to the face, where the first two points from the list\n","# determine the x-axis and its intersection with the line constructed by\n","# the last two points is the origin of the new coordinate system. Return\n","# origin, rad. rad represents the angle the x-axis of the new coordinate\n","# system needs to rotate counter-clockwisely in order to point at the same\n","# direction as the x-axis of the coordinate system of the entire screen.\n","# The 2 return values serve to calculate the normalized iris features\n","def get_face_plane(points3d):\n","  points2d = []\n","  for point3d in points3d:\n","    x, y, z = point3d\n","    points2d.append([x, y])\n","  pt1, pt2, pt3, pt4 = points2d\n","  xaxis = get_line(pt1, pt2)\n","  yaxis = get_line(pt3, pt4)\n","  origin = get_intersection(xaxis, yaxis)\n","  v = []\n","  for a, b in zip(pt1, pt2):\n","    v.append(b - a)\n","  rad = 2 * math.pi - get_ccw_angle(v)\n","  return origin, rad\n","\n","# Return a set of normalized iris features relative to the face coordinate\n","# system given the original iris features, origin and the counter-clockwise\n","# angle of the face coordinate system relative to the entire screen\n","def rotate(origin, points, angle):\n","  ox, oy = origin\n","  normalized_points = []\n","  for point in points:\n","    px, py, pz = point\n","    qx = ox + math.cos(angle) * (px - ox) - math.sin(angle) * (py - oy)\n","    qy = oy + math.sin(angle) * (px - ox) + math.cos(angle) * (py - oy)\n","    nx = qx - ox\n","    ny = qy - oy\n","    normalized_points.append([nx, ny])\n","  return normalized_points\n","\n","# Define indices of corresponding features in the 478 features list\n","irises = [469, 470, 471, 472, 474, 475, 476, 477]\n","face_cross = [226, 446, 9, 195]\n","\n","# Return the 6-dimensional face representation and the normalized iris features\n","# given a list of subject videos\n","def predict_and_normalize(videos):\n","  face_frames = []\n","  normalized_irises = []\n","  for video in videos:\n","    frames = video[\"features\"]\n","    for frame in frames:\n","      face_frame = [frame[i] for i in keypoints]\n","      face_frames.append(face_frame)\n","      irises_data = [frame[i] for i in irises]\n","      o, r = get_face_plane([frame[i] for i in face_cross])\n","      normalized_data = rotate(o, irises_data, r)\n","      normalized_irises.append(normalized_data)\n","  # The latent features are eventually converted because vae_encoder.predict()\n","  # only supports a 3D tensor as the input\n","  latent_features = vae_encoder.predict(face_frames)\n","  return latent_features, normalized_irises"],"metadata":{"id":"PN791R-m4tm4","executionInfo":{"status":"ok","timestamp":1658426339999,"user_tz":240,"elapsed":161,"user":{"displayName":"Shunji Wan","userId":"11173786701409222850"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Extract the facemesh features and rewrite them as latent features + normalized iris features"],"metadata":{"id":"H5ekr3BsUY_p"}},{"cell_type":"code","source":["# Extract a list of videos under an example subject. Note: json_data is already\n","# the content of the json file 'fwkruums'. This is just a shortcut to extract\n","# the value of the only key-value pair in the file\n","\n","subject_data = json_data['fwkruums']\n","print('There are ' + str(len(subject_data)) + ' videos for this subject')\n","\n","latent_features, normalized_irises = predict_and_normalize(subject_data)\n","\n","print(tf.shape(latent_features))\n","print(tf.shape(normalized_irises))\n","\n","# latent_features (3605 x 6) and normalized_irises (3605 x 8 x 2) are two \n","# separate lists. We may want to merge them together for convenience so that\n","# every element in aggregate_features contains all crucial information of the\n","# face in one frame\n","\n","aggregate_features = []\n","\n","for a, b in zip(latent_features, normalized_irises):\n","  aggregate_features.append([a, b])\n","\n","# We want to make a copy of subject_data and replace the 'features' content\n","# of every video with information of the latent features and normalized irises.\n","# We want to put these info back to the dictionary because eventually we need\n","# to sort the input for the incoming deep learning model according to the \n","# 'phase' and the 'block' attributes\n","\n","# This counter records number of frames processed.\n","# It updates per video processed\n","frames_counter = 0\n","\n","subject_data_copy = subject_data.copy()\n","\n","# Loop through videos\n","for video in subject_data_copy:\n","  # Check number of frames of the video\n","  frames_num = len(video['features'])\n","  # Index of the first element we want from aggregate_features\n","  head = frames_counter\n","  # Index of the first element we want from aggregate_features FOR THE NEXT VIDEO\n","  tail = head + frames_num\n","  # Rewrite the 'features' attribute\n","  video['features'] = [aggregate_features[i] for i in range(head, tail)]\n","  # Update counter\n","  frames_counter = tail"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I3w9ENeWUcWf","executionInfo":{"status":"ok","timestamp":1658426348615,"user_tz":240,"elapsed":5296,"user":{"displayName":"Shunji Wan","userId":"11173786701409222850"}},"outputId":"434dfa18-fdd0-4bad-879e-31ccc30562b7"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 129 videos for this subject\n","tf.Tensor([3605    6], shape=(2,), dtype=int32)\n","tf.Tensor([3605    8    2], shape=(3,), dtype=int32)\n"]}]},{"cell_type":"markdown","source":["# Extract the 90x14x(4+8x2) input tensor for the upcoming model"],"metadata":{"id":"0_XQaZjluL_6"}},{"cell_type":"code","source":["import random\n","\n","# Return a list of videos with the given 'block' attribute\n","def get_block_data(block_num):\n","  block_data = []\n","  for video in subject_data_copy:\n","    if video['block'] == block_num:\n","      block_data.append(video)\n","  return block_data\n","\n","# Return 2 lists of videos for calibration and test, respectively,\n","# given a list of videos share the same 'block' attribute\n","def get_ct_data(vlst):\n","  calibration_data = []\n","  test_data = []\n","  for video in vlst:\n","    if video['phase'] == 'calibration':\n","      calibration_data.append(video)\n","    else:\n","      test_data.append(video)\n","  return calibration_data, test_data\n","\n","# Get the calibration data of a block of videos as a reference\n","block_zero = get_block_data('0')\n","c_zero, t_zero = get_ct_data(block_zero)\n","calibration_pts = []\n","print(calibration_pts)\n","for video in c_zero:\n","  calibration_pts.append([video['x'], video['y']])\n","\n","print(calibration_pts)\n","\n","# Sort calibration data respective to the reference order\n","def sort_calibration(c_data):\n","  sorted_data = []\n","  for pt in calibration_pts:\n","    x = pt[0]\n","    y = pt[1]\n","    for video in c_data:\n","      if video['x'] == x and video['y'] == y:\n","        sorted_data.append(video)\n","      break\n","  return sorted_data\n","\n","\n","# Inputs list for the deep learning model\n","inputs = []\n","# Targets list corresponding to the inputs list\n","targets = []\n","\n","# Loop through blocks\n","for i in ['0', '1', '2']:\n","  block_data = get_block_data(i)\n","  c_data, t_data = get_ct_data(block_data)\n","  c_data = sort_calibration(c_data)\n","  # Loop through test videos first. The targets depend on the test videos NOT the calibration videos\n","  for t_video in t_data:\n","    # Declare an individual input list. There should be eventually 14 elements in it: 1 test video, all 13 calibration videos\n","    input = []\n","    t_frames = t_video['features']\n","    # Randomly frame selection\n","    input.append(random.choice(t_frames))\n","    # Note down the target gaze coordinate\n","    target = [int(t_video['x']), int(t_video['y'])]\n","    # For every test video, append all calibration videos info\n","    for c_video in c_data:\n","      c_frames = c_video['features']\n","      input.append(random.choice(c_frames))\n","    inputs.append(input)\n","    targets.append(target)\n"],"metadata":{"id":"wGLyNMhzW7pT","executionInfo":{"status":"ok","timestamp":1658426407109,"user_tz":240,"elapsed":141,"user":{"displayName":"Shunji Wan","userId":"11173786701409222850"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"229e88e0-7f81-4264-9f49-9dc482f1fd5f"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[]\n","[['10', '50'], ['10', '10'], ['90', '10'], ['50', '90'], ['30', '70'], ['50', '50'], ['50', '10'], ['90', '90'], ['70', '70'], ['70', '30'], ['10', '90'], ['90', '50'], ['30', '30']]\n"]}]}]}