{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IaP88N4vOMnw","executionInfo":{"status":"ok","timestamp":1676267097066,"user_tz":300,"elapsed":23961,"user":{"displayName":"Shunji Wan","userId":"11173786701409222850"}},"outputId":"051dc09e-4084-4785-a3b9-69d6f832e6ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mediapipe\n","  Downloading mediapipe-0.9.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.8/dist-packages (from mediapipe) (22.2.0)\n","Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.8/dist-packages (from mediapipe) (3.19.6)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from mediapipe) (23.1.21)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from mediapipe) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from mediapipe) (1.21.6)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from mediapipe) (3.2.2)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.8/dist-packages (from mediapipe) (4.6.0.66)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (3.0.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->mediapipe) (1.15.0)\n","Installing collected packages: mediapipe\n","Successfully installed mediapipe-0.9.1.0\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install mediapipe\n","import mediapipe as mp\n","import os\n","import json"]},{"cell_type":"markdown","source":["# **Load Trained VAE Model and MediaPipe**"],"metadata":{"id":"jJpyIk3QOcLB"}},{"cell_type":"code","source":["vae_path = '/content/drive/Shareddrives/Eye Tracking Research/Eye Tracking ML/vae_encoder/vae_2022-07-21_17:15:34'\n","vae_encoder = tf.keras.models.load_model(vae_path)\n","vae_encoder.summary()\n","\n","\n","mp_face_mesh = mp.solutions.face_mesh\n","left_eye_point = set(sum(mp_face_mesh.FACEMESH_LEFT_EYE, ()))\n","right_eye_point = set(sum(mp_face_mesh.FACEMESH_RIGHT_EYE, ()))\n","left_iris_point = set(sum(mp_face_mesh.FACEMESH_LEFT_IRIS, ()))\n","right_iris_point = set(sum(mp_face_mesh.FACEMESH_RIGHT_IRIS, ()))\n","face_oval_point = set(sum(mp_face_mesh.FACEMESH_FACE_OVAL, ()))\n","\n","keypoints = left_eye_point.union(right_eye_point).union(face_oval_point)\n","keypoints = keypoints.union([151, 9, 8, 168, 6, 197, 195, 5, 4])\n","\n","keypoints = sorted(list(keypoints))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9FJn37JNOZJ6","executionInfo":{"status":"ok","timestamp":1676267105607,"user_tz":300,"elapsed":5369,"user":{"displayName":"Shunji Wan","userId":"11173786701409222850"}},"outputId":"5fde65bb-65d5-466a-c8c1-c079f6586bab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"model_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," vae_encoder_input (InputLay  [(None, 77, 3)]          0         \n"," er)                                                             \n","                                                                 \n"," vae_flatten (Flatten)       (None, 231)               0         \n","                                                                 \n"," vae_dense_1 (Dense)         (None, 200)               46400     \n","                                                                 \n"," vae_dense_2 (Dense)         (None, 100)               20100     \n","                                                                 \n"," vae_dense_3 (Dense)         (None, 50)                5050      \n","                                                                 \n"," z_mean (Dense)              (None, 6)                 306       \n","                                                                 \n","=================================================================\n","Total params: 71,856\n","Trainable params: 71,856\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["# **Helper Functions**"],"metadata":{"id":"89hsrOO4O3g_"}},{"cell_type":"code","source":["import statistics\n","import math\n","import random\n","\n","# Order in which calibration data is to be sorted\n","calibration_pts = [['10', '50'], ['10', '10'], ['90', '10'], ['50', '90'],\n","                   ['30', '70'], ['50', '50'], ['50', '10'], ['90', '90'],\n","                   ['70', '70'], ['70', '30'], ['10', '90'], ['90', '50'],\n","                   ['30', '30']]\n","\n","# Define indices of corresponding features in the 478 features list\n","irises = [469, 470, 471, 472, 474, 475, 476, 477]\n","face_cross = [226, 446, 9, 195]\n","\n","def to_float(pts):\n","  output = []\n","  for pt in pts:\n","    x, y = pt\n","    output.append([float(x), float(y)])\n","  return output\n","\n","# Return coefficients a, b that represent the straight line \n","# constructed by the given points pt1, pt2 (y = ax + b)\n","def get_line(pt1, pt2):\n","  x1, y1 = pt1\n","  x2, y2 = pt2\n","  a = (y2 - y1) / (x2 - x1)\n","  b = y1 - (a * x1)\n","  return [a, b]\n","\n","# Return the coordinate of intersection of two straight lines\n","# l1, l2 in terms of [x, y]\n","def get_intersection(l1, l2):\n","  a1, b1 = l1\n","  a2, b2 = l2\n","  x = (b2 - b1) / (a1 - a2)\n","  y = a1 * x + b1\n","  return [x, y]\n","\n","# Return the angle that a vector needs to rotate counter-clockwisely\n","# in order to point at the same direction as the x-axis\n","def get_ccw_angle(vector):\n","  x, y = vector\n","  tan = y / x\n","  r = math.atan(tan)\n","  if x >= 0 and y > 0:\n","    pass\n","  elif x < 0 and y >= 0:\n","    r = r + math.pi\n","  elif x <= 0 and y < 0:\n","    r = r + math.pi\n","  elif x > 0 and y <= 0:\n","    r = r + 2 * math.pi\n","  else:\n","    r = 0\n","  return r\n","\n","# Given a list of 4 landmarks on the face mesh, construct a new coordinate\n","# system relative to the face, where the first two points from the list\n","# determine the x-axis and its intersection with the line constructed by\n","# the last two points is the origin of the new coordinate system. Return\n","# origin, rad. rad represents the angle the x-axis of the new coordinate\n","# system needs to rotate counter-clockwisely in order to point at the same\n","# direction as the x-axis of the coordinate system of the entire screen.\n","# The 2 return values serve to calculate the normalized iris features\n","def get_face_plane(points3d):\n","  points2d = []\n","  for point3d in points3d:\n","    x, y, z = point3d\n","    points2d.append([x, y])\n","  pt1, pt2, pt3, pt4 = points2d\n","  xaxis = get_line(pt1, pt2)\n","  yaxis = get_line(pt3, pt4)\n","  origin = get_intersection(xaxis, yaxis)\n","  v = []\n","  for a, b in zip(pt1, pt2):\n","    v.append(b - a)\n","  rad = 2 * math.pi - get_ccw_angle(v)\n","  return origin, rad\n","\n","# Return a set of normalized iris features relative to the face coordinate\n","# system given the original iris features, origin and the counter-clockwise\n","# angle of the face coordinate system relative to the entire screen\n","def rotate(origin, points, angle):\n","  ox, oy = origin\n","  normalized_points = []\n","  for point in points:\n","    px, py, pz = point\n","    qx = ox + math.cos(angle) * (px - ox) - math.sin(angle) * (py - oy)\n","    qy = oy + math.sin(angle) * (px - ox) + math.cos(angle) * (py - oy)\n","    nx = qx - ox\n","    ny = qy - oy\n","    normalized_points.append([nx, ny])\n","  return normalized_points\n","\n","# Return the 6-dimensional face representation and the normalized iris features\n","# given a list of subject videos\n","def predict_and_normalize(videos):\n","  face_frames = []\n","  normalized_irises = []\n","  for video in videos:\n","    frames = video[\"features\"]\n","    for frame in frames:\n","      face_frame = [frame[i] for i in keypoints]\n","      face_frames.append(face_frame)\n","      irises_data = [frame[i] for i in irises]\n","      o, r = get_face_plane([frame[i] for i in face_cross])\n","      normalized_data = rotate(o, irises_data, r)\n","      normalized_irises.append(normalized_data)\n","  # The latent features are eventually converted because vae_encoder.predict()\n","  # only supports a 3D tensor as the input\n","  latent_features = vae_encoder.predict(face_frames)\n","  latent_features = latent_features.tolist()\n","  return latent_features, normalized_irises\n","\n","# Return a list of videos with the given 'block' attribute\n","def get_block_data(block_num, subject_data):\n","  block_data = []\n","  for video in subject_data:\n","    if video['block'] == block_num:\n","      block_data.append(video)\n","  return block_data\n","\n","# Return 2 lists of videos for calibration and test, respectively,\n","# given a list of videos share the same 'block' attribute\n","def get_ct_data(vlst):\n","  calibration_data = []\n","  test_data = []\n","  for video in vlst:\n","    if video['phase'] == 'calibration':\n","      calibration_data.append(video)\n","    else:\n","      test_data.append(video)\n","  return calibration_data, test_data\n","\n","# Sort calibration data respective to the reference order\n","def sort_calibration(c_data):\n","  sorted_data = []\n","  for pt in calibration_pts:\n","    x = pt[0]\n","    y = pt[1]\n","    for video in c_data:\n","      if video['x'] == x and video['y'] == y:\n","        sorted_data.append(video)\n","  return sorted_data\n","\n","def flatten_features(features):\n","  output = []\n","  f = sum(features, [])\n","  for e in f:\n","    if type(e) is list:\n","      output = output + e\n","    else: output.append(e)\n","  return output"],"metadata":{"id":"gMQZbkWAO6nY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Main Processing Function**"],"metadata":{"id":"IpQCEjwku1vU"}},{"cell_type":"code","source":["# This giant function takes the json file of one individual subject as input\n","# and processes it. Eventually, it returns a list of inputs and its \n","# corresponding list of target gaze points.\n","\n","def get_inputs_targets(subject_name, subject_data):\n","\n","  print('Begin to process subject ' + subject_name + '...')\n","\n","  latent_features, normalized_irises = predict_and_normalize(subject_data)\n","\n","  # latent_features (3605 x 6) and normalized_irises (3605 x 8 x 2) are two \n","  # separate lists. We may want to merge them together for convenience so that\n","  # every element in aggregate_features contains all crucial information of the\n","  # face in one frame\n","\n","  aggregate_features = []\n","\n","  for a, b in zip(latent_features, normalized_irises):\n","    aggregate_features.append([a, b])\n","\n","  print('Face representation and normalized iris features sorted...')\n","\n","  # We want to make a copy of subject_data and replace the 'features' content\n","  # of every video with information of the latent features and normalized irises.\n","  # We want to put these info back to the dictionary because eventually we need\n","  # to sort the input for the incoming deep learning model according to the \n","  # 'phase' and the 'block' attributes\n","\n","  # This counter records number of frames processed.\n","  # It updates per video processed\n","  frames_counter = 0\n","\n","  subject_data_copy = subject_data.copy()\n","\n","  print('Made a copy of the subject ' + subject_name + '...')\n","\n","  # Loop through videos\n","  for video in subject_data_copy:\n","    # Check number of frames of the video\n","    frames_num = len(video['features'])\n","    # Index of the first element we want from aggregate_features\n","    head = frames_counter\n","    # Index of the first element we want from aggregate_features FOR THE NEXT VIDEO\n","    tail = head + frames_num\n","    # Rewrite the 'features' attribute\n","    video['features'] = [aggregate_features[i] for i in range(head, tail)]\n","    # Update counter\n","    frames_counter = tail\n","\n","  print('\\\"features\\\" attribute rewritten...')\n","\n","  trainX = []\n","  trainY = []\n","  testX = []\n","  testY = []\n","\n","  # Loop through blocks\n","  for i in ['0', '1', '2']:\n","    block_data = get_block_data(i, subject_data_copy)\n","    c_data, t_data = get_ct_data(block_data)\n","    # Check if there are 13 calibration videos for this block\n","    if len(c_data) < 13:\n","      # If not, print error messages and return\n","      print('ERROR MESSAGE: ' + subject_name + ' has insufficient number of calibration data in block ' + i + ': ' + str(len(c_data)))\n","      return False\n","\n","    c_data = sort_calibration(c_data)\n","    for c in c_data:\n","      c_frames = c['features']\n","      try:\n","        # Try random frame selection\n","        trainX.append(flatten_features(random.choice(c_frames)))\n","      except:\n","        # Catch the exception\n","        print('ERROR MESSAGE: ' + subject_name + ' has a calibration video with 0 frames')\n","        print('-----> block ' + i)\n","        continue\n","\n","    # Loop through test videos.\n","    for t_video in t_data:\n","      # Note down the target gaze coordinate\n","      target = [int(t_video['x']), int(t_video['y'])]\n","      t_frames = t_video['features']\n","      try:\n","        # Try random frame selection\n","        testX.append(flatten_features(random.choice(t_frames)))\n","        testY.append(target)\n","      except:\n","        # Catch the exception and go to the next test video\n","        x, y = target\n","        print('ERROR MESSAGE: ' + subject_name + ' has a test video with 0 frames')\n","        print('-----> block ' + i + ', (' + x + ', ' + y + ')')\n","        continue\n","    trainY = trainY + to_float(calibration_pts)\n","\n","  return trainX, trainY, testX, testY"],"metadata":{"id":"GGTthAHquzG8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Import Section**"],"metadata":{"id":"H5fLaCGNmiKE"}},{"cell_type":"code","source":["from keras import Model\n","from keras.layers import Layer\n","import keras.backend as K\n","from keras.layers import Input, Dense, SimpleRNN\n","from keras.models import Sequential\n","from keras.metrics import mean_squared_error"],"metadata":{"id":"wJdHm4xRmoG9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Add attention layer to the deep learning network\n","class attention(Layer):\n","    def __init__(self,**kwargs):\n","        super(attention,self).__init__(**kwargs)\n"," \n","    def build(self,input_shape):\n","        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1), \n","                               initializer='random_normal', trainable=True)\n","        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1), \n","                               initializer='zeros', trainable=True)        \n","        super(attention, self).build(input_shape)\n"," \n","    def call(self,x):\n","        # Alignment scores. Pass them through tanh function\n","        e = K.tanh(K.dot(x,self.W)+self.b)\n","        # Remove dimension of size 1\n","        e = K.squeeze(e, axis=-1)   \n","        # Compute the weights\n","        alpha = K.softmax(e)\n","        # Reshape to tensorFlow format\n","        alpha = K.expand_dims(alpha, axis=-1)\n","        # Compute the context vector\n","        context = x * alpha\n","        context = K.sum(context, axis=1)\n","        return context\n","\n","def create_RNN(hidden_units, dense_units, input_shape, activation):\n","  x=Input(shape=input_shape)\n","  RNN_layer = SimpleRNN(hidden_units, return_sequences=True, activation=activation)(x)\n","  attention_layer = attention()(RNN_layer)\n","  outputs=Dense(dense_units, trainable=True, activation=activation)(attention_layer)\n","  model=Model(x,outputs)\n","  model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"mean_squared_error\"])\n","  return model\n","\n","model_RNN = create_RNN(2, 1, (22, 1), 'tanh')\n","model_RNN.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XHy9tGvcmstr","executionInfo":{"status":"ok","timestamp":1676273130598,"user_tz":300,"elapsed":688,"user":{"displayName":"Shunji Wan","userId":"11173786701409222850"}},"outputId":"4de8eea2-c6bd-4cb4-da5e-e09eff86abee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_7 (InputLayer)        [(None, 22, 1)]           0         \n","                                                                 \n"," simple_rnn_10 (SimpleRNN)   (None, 22, 2)             8         \n","                                                                 \n"," attention_4 (attention)     (None, 2)                 24        \n","                                                                 \n"," dense_8 (Dense)             (None, 1)                 3         \n","                                                                 \n","=================================================================\n","Total params: 35\n","Trainable params: 35\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["json_path = '/content/drive/Shareddrives/Eye Tracking Research/Eye Tracking ML/json/'\n","all_json_files = os.listdir(json_path)\n","\n","json_data = {}\n","for filename in all_json_files:\n","  with open('/content/drive/Shareddrives/Eye Tracking Research/Eye Tracking ML/json/lyln56b2.json', 'r') as file:\n","    json_data = json.load(file)\n","\n","s_data = None\n","s_name = None\n","for name, data in json_data.items():\n","  s_name = name\n","  s_data = data\n","\n","trainX, trainY, testX, testY = get_inputs_targets(s_name, s_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bsIZhrbpTIuI","executionInfo":{"status":"ok","timestamp":1676273175696,"user_tz":300,"elapsed":40312,"user":{"displayName":"Shunji Wan","userId":"11173786701409222850"}},"outputId":"588487c5-51bb-4d73-e8b9-45e8a35443a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Begin to process subject lyln56b2...\n","129/129 [==============================] - 0s 2ms/step\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject lyln56b2...\n","\"features\" attribute rewritten...\n"]}]},{"cell_type":"code","source":["len(testY)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oWDqzpmBonER","executionInfo":{"status":"ok","timestamp":1676272743148,"user_tz":300,"elapsed":145,"user":{"displayName":"Shunji Wan","userId":"11173786701409222850"}},"outputId":"adf41048-5971-4c55-a074-c0e72e25fd4a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["90"]},"metadata":{},"execution_count":105}]},{"cell_type":"code","source":["model_RNN.fit(x=trainX, y=trainY, epochs=50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pBXZPKdnXBJe","executionInfo":{"status":"ok","timestamp":1676273214534,"user_tz":300,"elapsed":4528,"user":{"displayName":"Shunji Wan","userId":"11173786701409222850"}},"outputId":"2acec20c-ffb8-4ba7-f539-0db314e6769e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","2/2 [==============================] - 1s 9ms/step - loss: 3364.2676 - mean_squared_error: 3364.2676\n","Epoch 2/50\n","2/2 [==============================] - 0s 7ms/step - loss: 3363.5894 - mean_squared_error: 3363.5894\n","Epoch 3/50\n","2/2 [==============================] - 0s 11ms/step - loss: 3362.9006 - mean_squared_error: 3362.9006\n","Epoch 4/50\n","2/2 [==============================] - 0s 8ms/step - loss: 3362.2363 - mean_squared_error: 3362.2363\n","Epoch 5/50\n","2/2 [==============================] - 0s 8ms/step - loss: 3361.5645 - mean_squared_error: 3361.5645\n","Epoch 6/50\n","2/2 [==============================] - 0s 8ms/step - loss: 3360.8979 - mean_squared_error: 3360.8979\n","Epoch 7/50\n","2/2 [==============================] - 0s 9ms/step - loss: 3360.2231 - mean_squared_error: 3360.2231\n","Epoch 8/50\n","2/2 [==============================] - 0s 7ms/step - loss: 3359.5613 - mean_squared_error: 3359.5613\n","Epoch 9/50\n","2/2 [==============================] - 0s 8ms/step - loss: 3358.9019 - mean_squared_error: 3358.9019\n","Epoch 10/50\n","2/2 [==============================] - 0s 9ms/step - loss: 3358.2332 - mean_squared_error: 3358.2332\n","Epoch 11/50\n","2/2 [==============================] - 0s 7ms/step - loss: 3357.5681 - mean_squared_error: 3357.5681\n","Epoch 12/50\n","2/2 [==============================] - 0s 9ms/step - loss: 3356.9072 - mean_squared_error: 3356.9072\n","Epoch 13/50\n","2/2 [==============================] - 0s 12ms/step - loss: 3356.2517 - mean_squared_error: 3356.2517\n","Epoch 14/50\n","2/2 [==============================] - 0s 8ms/step - loss: 3355.5784 - mean_squared_error: 3355.5784\n","Epoch 15/50\n","2/2 [==============================] - 0s 8ms/step - loss: 3354.9165 - mean_squared_error: 3354.9165\n","Epoch 16/50\n","2/2 [==============================] - 0s 7ms/step - loss: 3354.2656 - mean_squared_error: 3354.2656\n","Epoch 17/50\n","2/2 [==============================] - 0s 9ms/step - loss: 3353.5957 - mean_squared_error: 3353.5957\n","Epoch 18/50\n","2/2 [==============================] - 0s 7ms/step - loss: 3352.9314 - mean_squared_error: 3352.9314\n","Epoch 19/50\n","2/2 [==============================] - 0s 8ms/step - loss: 3352.2671 - mean_squared_error: 3352.2671\n","Epoch 20/50\n","2/2 [==============================] - 0s 8ms/step - loss: 3351.6013 - mean_squared_error: 3351.6013\n","Epoch 21/50\n","2/2 [==============================] - 0s 9ms/step - loss: 3350.9250 - mean_squared_error: 3350.9250\n","Epoch 22/50\n","2/2 [==============================] - 0s 10ms/step - loss: 3350.2405 - mean_squared_error: 3350.2405\n","Epoch 23/50\n","2/2 [==============================] - 0s 8ms/step - loss: 3349.5583 - mean_squared_error: 3349.5583\n","Epoch 24/50\n","2/2 [==============================] - 0s 9ms/step - loss: 3348.8777 - mean_squared_error: 3348.8777\n","Epoch 25/50\n","2/2 [==============================] - 0s 11ms/step - loss: 3348.1665 - mean_squared_error: 3348.1665\n","Epoch 26/50\n","2/2 [==============================] - 0s 9ms/step - loss: 3347.4661 - mean_squared_error: 3347.4661\n","Epoch 27/50\n","2/2 [==============================] - 0s 9ms/step - loss: 3346.7644 - mean_squared_error: 3346.7644\n","Epoch 28/50\n","2/2 [==============================] - 0s 8ms/step - loss: 3346.0430 - mean_squared_error: 3346.0430\n","Epoch 29/50\n","2/2 [==============================] - 0s 8ms/step - loss: 3345.3269 - mean_squared_error: 3345.3269\n","Epoch 30/50\n","2/2 [==============================] - 0s 8ms/step - loss: 3344.6079 - mean_squared_error: 3344.6079\n","Epoch 31/50\n","2/2 [==============================] - 0s 20ms/step - loss: 3343.8682 - mean_squared_error: 3343.8682\n","Epoch 32/50\n","2/2 [==============================] - 0s 7ms/step - loss: 3343.1431 - mean_squared_error: 3343.1431\n","Epoch 33/50\n","2/2 [==============================] - 0s 10ms/step - loss: 3342.4011 - mean_squared_error: 3342.4011\n","Epoch 34/50\n","2/2 [==============================] - 0s 8ms/step - loss: 3341.6670 - mean_squared_error: 3341.6670\n","Epoch 35/50\n","2/2 [==============================] - 0s 14ms/step - loss: 3340.9187 - mean_squared_error: 3340.9187\n","Epoch 36/50\n","2/2 [==============================] - 0s 9ms/step - loss: 3340.1655 - mean_squared_error: 3340.1655\n","Epoch 37/50\n","2/2 [==============================] - 0s 10ms/step - loss: 3339.4180 - mean_squared_error: 3339.4180\n","Epoch 38/50\n","2/2 [==============================] - 0s 8ms/step - loss: 3338.6719 - mean_squared_error: 3338.6719\n","Epoch 39/50\n","2/2 [==============================] - 0s 7ms/step - loss: 3337.9028 - mean_squared_error: 3337.9028\n","Epoch 40/50\n","2/2 [==============================] - 0s 10ms/step - loss: 3337.1567 - mean_squared_error: 3337.1567\n","Epoch 41/50\n","2/2 [==============================] - 0s 8ms/step - loss: 3336.3867 - mean_squared_error: 3336.3867\n","Epoch 42/50\n","2/2 [==============================] - 0s 9ms/step - loss: 3335.6277 - mean_squared_error: 3335.6277\n","Epoch 43/50\n","2/2 [==============================] - 0s 11ms/step - loss: 3334.8638 - mean_squared_error: 3334.8638\n","Epoch 44/50\n","2/2 [==============================] - 0s 9ms/step - loss: 3334.1074 - mean_squared_error: 3334.1074\n","Epoch 45/50\n","2/2 [==============================] - 0s 8ms/step - loss: 3333.3208 - mean_squared_error: 3333.3208\n","Epoch 46/50\n","2/2 [==============================] - 0s 12ms/step - loss: 3332.5603 - mean_squared_error: 3332.5603\n","Epoch 47/50\n","2/2 [==============================] - 0s 8ms/step - loss: 3331.7900 - mean_squared_error: 3331.7900\n","Epoch 48/50\n","2/2 [==============================] - 0s 9ms/step - loss: 3331.0127 - mean_squared_error: 3331.0127\n","Epoch 49/50\n","2/2 [==============================] - 0s 7ms/step - loss: 3330.2161 - mean_squared_error: 3330.2161\n","Epoch 50/50\n","2/2 [==============================] - 0s 8ms/step - loss: 3329.4539 - mean_squared_error: 3329.4539\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f9fda3028e0>"]},"metadata":{},"execution_count":117}]},{"cell_type":"code","source":["train_mse = model_RNN.evaluate(trainX, trainY)\n","test_mse = model_RNN.evaluate(testX, testY)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1tj5r9Dpgi8Q","executionInfo":{"status":"ok","timestamp":1676273223974,"user_tz":300,"elapsed":973,"user":{"displayName":"Shunji Wan","userId":"11173786701409222850"}},"outputId":"fe9635c5-de45-4127-fb87-f5a829e4e41d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2/2 [==============================] - 0s 11ms/step - loss: 3328.7393 - mean_squared_error: 3328.7393\n","3/3 [==============================] - 0s 4ms/step - loss: 3202.5608 - mean_squared_error: 3202.5608\n"]}]}]}