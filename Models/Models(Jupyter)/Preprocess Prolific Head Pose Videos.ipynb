{"cells":[{"cell_type":"markdown","source":["# Preprocessing Steps for Head Pose Videos\n","\n","This notebook is for extracting mesh features from `.webm` videos generated by the head pose recording experiment. The result of running this pipeline is a `.json` file for each subject/session in the head pose experiment."],"metadata":{"id":"3ll-npHHozPw"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5499,"status":"ok","timestamp":1658422438138,"user":{"displayName":"Oliravan Eswaramoorthy","userId":"13611592422418692717"},"user_tz":240},"id":"cfqjWhd5yEcu","outputId":"5b250fc1-ce06-44da-9dea-c4db3a397eaa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: mediapipe in /usr/local/lib/python3.7/dist-packages (0.8.10.1)\n","Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.17.3)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.6.0.66)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.21.6)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.2.2)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.1.0)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (21.4.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<4,>=3.11->mediapipe) (1.15.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (0.11.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mediapipe) (4.1.1)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import numpy as np\n","import cv2\n","import os\n","import fnmatch\n","import json\n","!pip install mediapipe\n","import mediapipe as mp\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"yWZgrC25wQG3"},"source":["# Configure MediaPipe FaceMesh\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8wLLQ1YZvZGF"},"outputs":[],"source":["mp_face_mesh = mp.solutions.face_mesh\n","face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, refine_landmarks=True)"]},{"cell_type":"markdown","metadata":{"id":"2x-nnHcut9-_"},"source":["# Process Video Pipeline"]},{"cell_type":"markdown","source":["Define the function to take in a `path` to a video file and output an array of shape (`frames`, `landmarks`, `3`), where `frames` is the number of frames in the video file, `landmarks` is the number of landmarks extracted by the face mesh model, and `3` corresponds to the `x`, `y`, and `z` coordinates for each landmark."],"metadata":{"id":"S21h5wcMpeAx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DedTOFtRuDF_"},"outputs":[],"source":["def extract_mesh_from_video(path):\n","  # open a video file for video capturing\n","  cap = cv2.VideoCapture(path)\n","  out = []\n","  \n","  # to see if video capturing has been initialized\n","  while(cap.isOpened()):\n","    # return (1) if any frames grabbed (2) grabbed image (empty if ret is false)\n","    ret, frame = cap.read()\n","    # Q: why frame could be none?\n","    if frame is not None:\n","      # process an RGB image and returns the face landmarks on each detected face\n","      results = face_mesh.process(frame)\n","      # check if any faces detected\n","      if not results.multi_face_landmarks:\n","        continue\n","      landmarks = results.multi_face_landmarks[0].landmark\n","      # store landmarks as an array of arrays\n","      lm = [[a.x, a.y, a.z] for a in landmarks]\n","      # 3D tensor that stores landmarks frame by frame\n","      out.append(lm)\n","    else:\n","      break\n","\n","  if len(out) > 0:\n","    out = np.reshape(np.array(out), (len(out), -1, 3)).tolist()\n","  return out"]},{"cell_type":"markdown","source":["Run through all of the video files in the folder and process their data. If `.json` output already exists for the subject ID, then skip processing. (To overwrite existing data, delete the corresponding `.json` file before running this step.)"],"metadata":{"id":"JbGpX4Hnp7EL"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18664,"status":"ok","timestamp":1658422456794,"user":{"displayName":"Oliravan Eswaramoorthy","userId":"13611592422418692717"},"user_tz":240},"id":"soMq474-143F","outputId":"4eae9d87-4f6d-4416-b430-ff4c4df4f976"},"outputs":[{"output_type":"stream","name":"stdout","text":["17\n","{'8wplohk7', 'duavtnj5', 'q2rv1bj0', '0647xw1u', 'ryvtw1w8', 'dl75d10s', 'c17pt575', '1nqsmw2a', 'z98nk7pa', 'epsxz1ys', '76qjhxwz', '459wtwlr', '4n48phr9', 'to5p13vd', '7ladzmrj', '8vf1z9hb'}\n","8wplohk7\n","duavtnj5\n","q2rv1bj0\n","0647xw1u\n","ryvtw1w8\n","dl75d10s\n","c17pt575\n","1nqsmw2a\n","z98nk7pa\n","epsxz1ys\n","76qjhxwz\n","459wtwlr\n","4n48phr9\n","to5p13vd\n","7ladzmrj\n","8vf1z9hb\n"]}],"source":["# get the path of the webm file\n","path = '/content/drive/Shareddrives/URSI 2022/Eye Tracking ML/prolific_eye_data_experiment/vae_test_webm/'\n","\n","# store all the file directories\n","all_files = os.listdir(path)\n","print(len(all_files))\n","\n","# get unique subjects\n","unique_subjects = set([filepath.split('_')[0] for filepath in os.listdir(path)])\n","print(unique_subjects)\n","\n","for subject in unique_subjects:\n","  all_data = {}\n","\n","  print(subject)\n","\n","  # check if the subject json file already exists. if so, skip the remainning body\n","  if os.path.isfile('/content/drive/Shareddrives/URSI 2022/Eye Tracking ML/prolific_eye_data_experiment/vae_test_json/'+subject+'.json'):\n","    continue\n","   \n","  subject_data = []\n","  # go through all file directories in the webm file, find those that start with the subject name\n","  subject_files = fnmatch.filter(all_files, subject+'*')\n","  # manage every single file directory that starts with the subject name\n","  for filename in subject_files:\n","    # transform file name into an array\n","    fileinfo = filename.replace('.','_').split('_')\n","    # store relevant values\n","    subject = fileinfo[0]\n","    meshfeatures = extract_mesh_from_video(path + filename)\n","    # create and append a dictionary to the exisiting array\n","    subject_data.append({\n","        'features': meshfeatures \n","    })\n","  # once the last for loop is over, assign the subject_data array as the value to the subject key\n","  all_data[subject] = subject_data\n","\n","  # export the json file for the subject to the drive\n","  with open('/content/drive/Shareddrives/URSI 2022/Eye Tracking ML/prolific_eye_data_experiment/vae_test_json/'+subject+'.json', 'w') as file:\n","    json.dump(all_data, file)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1658422456795,"user":{"displayName":"Oliravan Eswaramoorthy","userId":"13611592422418692717"},"user_tz":240},"id":"UneHehSTmjrl","outputId":"fa0d73d7-277e-40ce-ed2c-abf0a3497bef"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'0647xw1u',\n"," '1nqsmw2a',\n"," '459wtwlr',\n"," '4n48phr9',\n"," '76qjhxwz',\n"," '7ladzmrj',\n"," '8vf1z9hb',\n"," '8wplohk7',\n"," 'c17pt575',\n"," 'dl75d10s',\n"," 'duavtnj5',\n"," 'epsxz1ys',\n"," 'q2rv1bj0',\n"," 'ryvtw1w8',\n"," 'to5p13vd',\n"," 'z98nk7pa'}"]},"metadata":{},"execution_count":5}],"source":["unique_subjects"]}],"metadata":{"colab":{"provenance":[{"file_id":"1q6kPSThszC5ViIAbAaKmyFXiOyIK6eqV","timestamp":1658400346662}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}