{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36315,"status":"ok","timestamp":1657662956885,"user":{"displayName":"Shunji Wan","userId":"11173786701409222850"},"user_tz":240},"id":"cfqjWhd5yEcu","outputId":"c0e65a59-8720-45ba-ffbe-8cbbea01181d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mediapipe\n","  Downloading mediapipe-0.8.10.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.9 MB)\n","\u001b[K     |████████████████████████████████| 32.9 MB 168 kB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.21.6)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.1.2.30)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.1.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.2.2)\n","Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.17.3)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (21.4.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<4,>=3.11->mediapipe) (1.15.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (1.4.3)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (3.0.9)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (0.11.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mediapipe) (4.1.1)\n","Installing collected packages: mediapipe\n","Successfully installed mediapipe-0.8.10.1\n","Mounted at /content/drive\n"]}],"source":["import numpy as np\n"," import cv2\n","import os\n","import fnmatch\n","import json\n","import tensorflow as tf\n","import random\n","from google.colab.patches import cv2_imshow\n","!pip install mediapipe\n","import mediapipe as mp\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"yWZgrC25wQG3"},"source":["# Configure MediaPipe FaceMesh\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8wLLQ1YZvZGF"},"outputs":[],"source":["mp_face_mesh = mp.solutions.face_mesh\n","mp_drawing = mp.solutions.drawing_utils\n","mp_drawing_styles = mp.solutions.drawing_styles\n","face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, refine_landmarks=True)"]},{"cell_type":"markdown","metadata":{"id":"2x-nnHcut9-_"},"source":["# Process Video Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DedTOFtRuDF_"},"outputs":[],"source":["def extract_mesh_from_video(path):\n","  # open a video file for video capturing\n","  cap = cv2.VideoCapture(path)\n","  out = []\n","  \n","  # to see if video capturing has been initialized\n","  while(cap.isOpened()):\n","    # return (1) if any frames grabbed (2) grabbed image (empty if ret is false)\n","    ret, frame = cap.read()\n","    # Q: why frame could be none?\n","    if frame is not None:\n","      # process an RGB image and returns the face landmarks on each detected face\n","      results = face_mesh.process(frame)\n","      # check if any faces detected\n","      if not results.multi_face_landmarks:\n","        continue\n","      landmarks = results.multi_face_landmarks[0].landmark\n","      # store landmarks as an array of arrays\n","      lm = [[a.x, a.y, a.z] for a in landmarks]\n","      # 3D tensor that stores landmarks frame by frame\n","      out.append(lm)\n","    else:\n","      break\n","\n","  if len(out) > 0:\n","    out = np.reshape(np.array(out), (len(out), -1, 3)).tolist()\n","  return out"]},{"cell_type":"code","source":["example_landmarks_data = extract_mesh_from_video(\"/content/drive/Shareddrives/URSI 2022/Eye Tracking ML/webm/v2sfzuft_2_test_95_8.webm\")\n","tf.shape(example_landmarks_data)\n","\n","# each video has a shape of 32x478x3\n","# 32 frames, 478 landmarks, coordinates (x, y, z)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gYhIH6m2jFvd","executionInfo":{"status":"ok","timestamp":1657662972392,"user_tz":240,"elapsed":10830,"user":{"displayName":"Shunji Wan","userId":"11173786701409222850"}},"outputId":"95f43b51-1921-4264-ecbc-fb02a4faf484"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3,), dtype=int32, numpy=array([ 32, 478,   3], dtype=int32)>"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":137,"status":"ok","timestamp":1657662976657,"user":{"displayName":"Shunji Wan","userId":"11173786701409222850"},"user_tz":240},"id":"UneHehSTmjrl","outputId":"b9c89e2f-5cfe-4fa2-fb63-294eece7393b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'1lzaw0tb',\n"," '27o23haf',\n"," '7asl4wbk',\n"," 'c4g2mw61',\n"," 'fwkruums',\n"," 'g1klo888',\n"," 'j72zjd8w',\n"," 'k6yrzzo1',\n"," 'lyln56b2',\n"," 'v2sfzuft'}"]},"metadata":{},"execution_count":6}],"source":["unique_subjects"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":539,"status":"ok","timestamp":1657662974348,"user":{"displayName":"Shunji Wan","userId":"11173786701409222850"},"user_tz":240},"id":"soMq474-143F","outputId":"8830222e-c0d4-4ce7-ddb6-a683c9376cc9"},"outputs":[{"output_type":"stream","name":"stdout","text":["1277\n","{'j72zjd8w', '27o23haf', 'lyln56b2', 'c4g2mw61', '7asl4wbk', 'v2sfzuft', 'fwkruums', 'g1klo888', '1lzaw0tb', 'k6yrzzo1'}\n","j72zjd8w\n","27o23haf\n","lyln56b2\n","c4g2mw61\n","7asl4wbk\n","v2sfzuft\n","fwkruums\n","g1klo888\n","1lzaw0tb\n","k6yrzzo1\n"]}],"source":["# get the path of the webm file\n","path = '/content/drive/Shareddrives/URSI 2022/Eye Tracking ML/webm/'\n","\n","# store all the file directories\n","all_files = os.listdir(path)\n","print(len(all_files))\n","\n","# get unique subjects\n","unique_subjects = set([filepath.split('_')[0] for filepath in os.listdir(path)])\n","print(unique_subjects)\n","\n","for subject in unique_subjects:\n","  all_data = {}\n","\n","  print(subject)\n","\n","  # check if the subject json file already exists. if so, skip the remainning body\n","  if os.path.isfile('/content/drive/Shareddrives/URSI 2022/Eye Tracking ML/json/'+subject+'.json'):\n","    continue\n","   \n","  subject_data = []\n","  # go through all file directories in the webm file, find those that start with the subject name\n","  subject_files = fnmatch.filter(all_files, subject+'*')\n","  # manage every single file directory that starts with the subject name\n","  for filename in subject_files:\n","    # transform file name into an array\n","    fileinfo = filename.replace('.','_').split('_')\n","    # store relevant values\n","    subject = fileinfo[0]\n","    block = fileinfo[1]\n","    phase = fileinfo[2]\n","    x = fileinfo[3]\n","    y = fileinfo[4]\n","    meshfeatures = extract_mesh_from_video(path + filename)\n","    # create and append a dictionary to the exisiting array\n","    subject_data.append({\n","        'block': block,\n","        'phase': phase,\n","        'x': x,\n","        'y': y,\n","        'features': meshfeatures \n","    })\n","  # once the last for loop is over, assign the subject_data array as the value to the subject key\n","  all_data[subject] = subject_data\n","\n","  # export the json file for the subject to the drive\n","  with open('/content/drive/Shareddrives/URSI 2022/Eye Tracking ML/json/'+subject+'.json', 'w') as file:\n","    json.dump(all_data, file)"]},{"cell_type":"markdown","metadata":{"id":"jiyfMeCCzHbQ"},"source":["# Load JSON"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vGKz93YfzKkn"},"outputs":[],"source":["json_path = '/content/drive/Shareddrives/URSI 2022/Eye Tracking ML/json/'\n","all_json_files = os.listdir(json_path)\n","\n","json_data = {}\n","for filename in all_json_files:\n","  with open('/content/drive/Shareddrives/URSI 2022/Eye Tracking ML/json/'+filename, 'r') as file:\n","    s_data = json.load(file)\n","    json_data = {**json_data, **s_data}\n","\n","# every element in json_data is the info of a video\n","# {\n","#     \"lyln56b2\": [\n","#                  {\n","#                     \"block\": ...,\n","#                     \"phase\": ...,\n","#                     \"x\": ...,\n","#                     \"y\": ...,\n","#                     \"features\": ...,\n","#                  },\n","#                  {...},\n","#                  {...},\n","#                  ...\n","#     ]\n","# }"]},{"cell_type":"markdown","source":["# Ridge Regression Model"],"metadata":{"id":"cA25ehxMm71I"}},{"cell_type":"code","source":["from sklearn.linear_model import Ridge\n","import matplotlib.pyplot as plt\n","import statistics\n","import math\n","\n","def is_calibration_data(dict):\n","  if dict[\"phase\"] == \"calibration\":\n","    return True\n","  else: return False\n","\n","def is_test_data(dict):\n","  if dict[\"phase\"] == \"test\":\n","    return True\n","  else: return False\n","\n","# def get_tc_data(dict):\n","#   all_videos_data = []\n","#   for key, value in dict.items():\n","#     for per_video_data in value:\n","#       all_videos_data.append(per_video_data)\n","#   calibration_data = list(filter(is_calibration_data, all_videos_data))\n","#   test_data = list(filter(is_test_data, all_videos_data))\n","#   return calibration_data, test_data\n","\n","def split_and_flatten_data(v_list):\n","  predictor_data = []\n","  output_data = []\n","  for video in v_list:\n","    coor = [int(video[\"x\"]), int(video[\"y\"])]\n","    features = video[\"features\"]\n","    for feature in features:\n","      predictor_data.append(feature)\n","      output_data.append(coor)\n","  predictor_data = np.reshape(np.array(predictor_data), (-1, 478*3)).tolist()\n","  return predictor_data, output_data\n","\n","def split_xy(lst_of_coor):\n","  x = []\n","  y = []\n","  for coor in lst_of_coor:\n","    x.append(coor[0])\n","    y.append(coor[1])\n","  x = np.array(x)\n","  y = np.array(y)\n","  return x, y\n","\n","def draw_one_subject(lst_of_coor):\n","  x, y = split_xy(lst_of_coor)\n","  plt.scatter(x, y)\n","\n","def is_block_n_data(dict, block_name):\n","  if dict[\"block\"] == block_name:\n","    return True\n","  else: return False\n"],"metadata":{"id":"ys2N7MeNnzDl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sympy import Point3D, Line3D\n","\n","def get_centroid(lst):\n","  pt1 = lst[0]\n","  pt2 = lst[1]\n","  pt3 = lst[2]\n","  pt4 = lst[3]\n","  x1, y1, z1 = pt1\n","  x2, y2, z2 = pt2\n","  x3, y3, z3 = pt3\n","  x4, y4, z4 = pt4\n","  p1, p2 = Point3D(x1, y1, z1), Point3D(x2, y2, z2)\n","  p3, p4 = Point3D(x3, y3, z3), Point3D(x4, y4, z4)\n","  l1 = Line3D(p1, p2)\n","  l2 = Line3D(p3, p4)\n","  c = intersection(l1, l2)[0]\n","  x, y, z = c.x, c.y, c.z\n","  return [x, y, z]\n","\n","\n","# get_centroid([lst[i] for i in [226, 244, 223, 230]]),\n","#              get_centroid([lst[i] for i in [359, 463, 257, 253]]),\n","\n","def landmark_filter(lst):\n","  lm_lst =  [lst[i] for i in [13, 19, 234, 454, 10, 152]]\n","  centroid_lst = [\n","             get_centroid([lst[i] for i in [474, 475, 476, 477]]),\n","             get_centroid([lst[i] for i in [469, 470, 471, 472]])]\n","  return lm_lst.extend(centroid_lst)\n","\n","def split_and_flatten_data(v_list):\n","  predictor_data = []\n","  output_data = []\n","  for video in v_list:\n","    coor = [int(video[\"x\"]), int(video[\"y\"])]\n","    features = video[\"features\"]\n","    for feature in features:\n","      feature = landmark_filter(feature)\n","      predictor_data.append(feature)\n","      output_data.append(coor)\n","  predictor_data = np.reshape(np.array(predictor_data), (-1, len(feature)*3)).tolist()\n","  return predictor_data, output_data\n","\n","model_data = {}\n","\n","for key, value in json_data.items():\n","  calibration_data = list(filter(is_calibration_data, value))\n","  test_data = list(filter(is_test_data, value))\n","  c_training_data, c_target_data = split_and_flatten_data(calibration_data)\n","  t_predictor_data, t_target_data = split_and_flatten_data(test_data)\n","\n","  ridge_model = Ridge()\n","  ridge_model.fit(c_training_data, c_target_data)\n","  predictions = ridge_model.predict(t_predictor_data)\n","\n","  residual_distance = []\n","  tx, ty = split_xy(t_target_data)\n","  px, py = split_xy(predictions)\n","  residual_x = px - tx\n","  residual_y = py - ty\n","  for i in range(len(predictions)):\n","    d = math.sqrt(residual_x[i]**2 + residual_y[i]**2)\n","    residual_distance.append(d)\n","  mean_d = statistics.mean(residual_distance)\n","\n","  model_data[key] = {\n","      \"calibration data\": calibration_data,\n","      \"test data\": t_target_data,\n","      \"predicted data\": predictions,\n","      \"residual d\": residual_distance,\n","      \"mean d\": mean_d,\n","      \"residual x\": residual_x,\n","      \"residual y\": residual_y \n","  }\n"],"metadata":{"id":"0Rt9bkv541TA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for key, value in model_data.items():\n","#     print(value[\"mean d\"])\n","\n","# test_data = []\n","# predictions = []\n","# residual_x = []\n","# residual_y = []\n","# residual_d = []\n","\n","# for key, value in model_data.items():\n","#   test_data.append(value[\"test data\"])\n","#   predictions.append(value[\"predicted data\"])\n","#   residual_x.append(value[\"residual x\"])\n","#   residual_y.append(value[\"residual y\"])\n","#   residual_d.append(value[\"residual d\"])\n","\n","# test_data = np.concatenate(test_data).tolist()\n","# predictions = np.concatenate(predictions).tolist()\n","\n","# draw_one_subject(predictions)\n","# draw_one_subject(test_data)\n","# plt.show()\n","\n","# residual_x = np.concatenate(residual_x).tolist()\n","# plt.hist(residual_x)\n","# plt.show()\n","\n","# residual_y = np.concatenate(residual_y).tolist()\n","# plt.hist(residual_y)\n","# plt.show()\n","\n","# residual_d = np.concatenate(residual_d).tolist()\n","# plt.hist(residual_d)\n","# plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":236},"id":"oVmXZ3hGukhk","executionInfo":{"status":"error","timestamp":1657229290630,"user_tz":240,"elapsed":510,"user":{"displayName":"Shunji Wan","userId":"11173786701409222850"}},"outputId":"03013c61-7be6-445c-afb1-b49509f56bd2"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-fcfe3380bf29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mean d\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model_data' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"69ZonsVGz_o5"},"source":["# Simple Keras Model on One Subject"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tXU2VJ0OCbmB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657662982357,"user_tz":240,"elapsed":132,"user":{"displayName":"Shunji Wan","userId":"11173786701409222850"}},"outputId":"02645a95-ff5e-4008-a148-3dd4acfc154c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{469, 470, 471, 472}"]},"metadata":{},"execution_count":7}],"source":["left_eye_point = set(sum(mp_face_mesh.FACEMESH_LEFT_EYE, ()))\n","right_eye_point = set(sum(mp_face_mesh.FACEMESH_RIGHT_EYE, ()))\n","left_iris_point = set(sum(mp_face_mesh.FACEMESH_LEFT_IRIS, ()))\n","right_iris_point = set(sum(mp_face_mesh.FACEMESH_RIGHT_IRIS, ()))\n","\n","keypoints = left_eye_point.union(right_eye_point).union(left_iris_point).union(right_iris_point)\n","\n","keypoints = sorted(list(keypoints))\n","\n","left_iris_point\n","right_iris_point "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NO8SQ6TV0gq8"},"outputs":[],"source":["train_x = []\n","train_y = []\n","\n","val_x = []\n","val_y = []\n","\n","for subject in json_data:\n","  subject_data = json_data[subject];\n","  train_vids = random.sample(range(0,len(subject_data)), 100)\n","  for idx, video in enumerate(subject_data):\n","    for all_features in video['features']:\n","    #all_features = video['features'][0] # 0 picks the first frame per video. change this eventually?\n","      if idx in train_vids:\n","        train_x.append([all_features[i] for i in keypoints])\n","        train_y.append([int(video['x'])/100, int(video['y'])/100])\n","      else:\n","        val_x.append([all_features[i] for i in keypoints])\n","        val_y.append([int(video['x'])/100, int(video['y'])/100])"]},{"cell_type":"code","source":["train_y[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C0Rho45HbXbx","executionInfo":{"status":"ok","timestamp":1656337878407,"user_tz":240,"elapsed":135,"user":{"displayName":"Joshua de Leeuw","userId":"08854274972818186046"}},"outputId":"f9dc28fb-c672-4a52-e579-80d123fb247e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.22, 0.32]"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":882,"status":"ok","timestamp":1655820322754,"user":{"displayName":"Joshua de Leeuw","userId":"08854274972818186046"},"user_tz":240},"id":"2DIDUZ381VRa","outputId":"2701981a-fabb-4f27-d2a2-4b015d405e94"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 40, 3)]      0           []                               \n","                                                                                                  \n"," flatten (Flatten)              (None, 120)          0           ['input_1[0][0]']                \n","                                                                                                  \n"," dense (Dense)                  (None, 400)          48400       ['flatten[0][0]']                \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 520)          0           ['flatten[0][0]',                \n","                                                                  'dense[0][0]']                  \n","                                                                                                  \n"," dropout (Dropout)              (None, 520)          0           ['concatenate[0][0]']            \n","                                                                                                  \n"," dense_1 (Dense)                (None, 250)          130250      ['dropout[0][0]']                \n","                                                                                                  \n"," concatenate_1 (Concatenate)    (None, 770)          0           ['flatten[0][0]',                \n","                                                                  'dense[0][0]',                  \n","                                                                  'dense_1[0][0]']                \n","                                                                                                  \n"," dropout_1 (Dropout)            (None, 770)          0           ['concatenate_1[0][0]']          \n","                                                                                                  \n"," dense_2 (Dense)                (None, 2)            1542        ['dropout_1[0][0]']              \n","                                                                                                  \n","==================================================================================================\n","Total params: 180,192\n","Trainable params: 180,192\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["# ideas\n","# - try sharing the weights between the eyes to improve training?\n","# - error in y seems to be larger than error in x; is there a way to scale y values? normalization?\n","# - add calibration data as model input\n","# - try learning a generative model of face location by compressing eye (but NOT iris) data through something like a VAE.\n","#   then learn generative model of iris within? or maybe just learn together and hope VAE can separate dimensions out?\n","#   could even train the generative model on webcam face datasets. don't need this specific data.\n","# - check literature for solutions\n","\n","model_inputs = tf.keras.Input(shape=(40,3))\n","model_flatten = tf.keras.layers.Flatten()(model_inputs)\n","model_dense_1 = tf.keras.layers.Dense(units=400, activation=\"relu\")(model_flatten)\n","model_res_1 = tf.keras.layers.Concatenate()([model_flatten, model_dense_1])\n","model_dropout_1 = tf.keras.layers.Dropout(0.25)(model_res_1)\n","model_dense_2 = tf.keras.layers.Dense(units=250, activation=\"relu\")(model_dropout_1)\n","model_res_2 = tf.keras.layers.Concatenate()([model_flatten, model_dense_1, model_dense_2])\n","model_dropout_2 = tf.keras.layers.Dropout(0.25)(model_res_2)\n","model_outputs = tf.keras.layers.Dense(units=2, activation=None)(model_dropout_2)\n","\n","model = tf.keras.Model(inputs=model_inputs, outputs=model_outputs)\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWZN0kL94KJh"},"outputs":[],"source":["model.compile(optimizer='nadam', loss='mean_absolute_error', metrics=[tf.keras.metrics.mean_absolute_error])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5U_0Ozj4MGI"},"outputs":[],"source":["model.fit(x=train_x, y=train_y, epochs=100, validation_data = (val_x, val_y))"]},{"cell_type":"markdown","metadata":{"id":"shmh6K5N2s8V"},"source":["# a VAE for compressing the head/eye position"]},{"cell_type":"code","source":["class Sampling(tf.keras.layers.Layer):\n","    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a mesh.\"\"\"\n","\n","    def call(self, inputs):\n","        z_mean, z_log_var = inputs\n","        batch = tf.shape(z_mean)[0]\n","        dim = tf.shape(z_mean)[1]\n","        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n","        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"],"metadata":{"id":"pL3WA6gSQikX"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PJCC7eZD22Ej"},"outputs":[],"source":["latent_dim = 2\n","\n","vae_encoder_inputs = tf.keras.Input(shape=(40,3), name=\"vae_encoder_input\")\n","vae_encoder_flatten = tf.keras.layers.Flatten(name=\"vae_flatten\")(vae_encoder_inputs)\n","vae_encoder_dense_1 = tf.keras.layers.Dense(units=200, activation=\"relu\", name=\"vae_dense_1\")(vae_encoder_flatten)\n","vae_encoder_dense_2 = tf.keras.layers.Dense(units=50, activation=\"relu\", name=\"vae_dense_2\")(vae_encoder_dense_1)\n","z_mean = tf.keras.layers.Dense(units=latent_dim, name=\"z_mean\")(vae_encoder_dense_2)\n","z_log_var = tf.keras.layers.Dense(units=latent_dim, name=\"z_log_var\")(vae_encoder_dense_2)\n","z = Sampling(name=\"vae_sampling\")([z_mean, z_log_var])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"evX65avucRik"},"outputs":[],"source":["vae_decoder_dense_1 = tf.keras.layers.Dense(50, activation=\"relu\", name=\"vae_decoder_dense_1\")(z)\n","vae_decoder_dense_2 = tf.keras.layers.Dense(200, activation=\"relu\", name=\"vae_decoder_dense_2\")(vae_decoder_dense_1)\n","vae_decoder_dense_3 = tf.keras.layers.Dense(120, activation=\"sigmoid\", name=\"vae_decoder_dense_3\")(vae_decoder_dense_2)\n","vae_decoder_outputs = tf.keras.layers.Reshape((40,3), name=\"vae_decoder_reshape\")(vae_decoder_dense_3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ceWwjkV59oUa"},"outputs":[],"source":["vae = tf.keras.Model(inputs=vae_encoder_inputs, outputs=vae_decoder_outputs, name=\"VAE\")\n","\n","#kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n","#vae.add_loss(kl_loss)"]},{"cell_type":"code","source":["vae.compile(optimizer=\"adam\", loss=\"mean_squared_error\")"],"metadata":{"id":"Pea-z3JZR5U3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vae.losses"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-LMsHv4AKR9I","executionInfo":{"status":"ok","timestamp":1656269399514,"user_tz":240,"elapsed":207,"user":{"displayName":"Joshua de Leeuw","userId":"08854274972818186046"}},"outputId":"3f189d6e-6548-405c-a526-54c79f7271eb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["vae.fit(x=train_x, y=train_x, epochs=25)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JnRCrj6MwvbT","executionInfo":{"status":"ok","timestamp":1656269555137,"user_tz":240,"elapsed":153362,"user":{"displayName":"Joshua de Leeuw","userId":"08854274972818186046"}},"outputId":"ddc99d1c-3547-4aee-b232-5963f9453df6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n","966/966 [==============================] - 4s 4ms/step - loss: 0.0029\n","Epoch 2/25\n","966/966 [==============================] - 3s 4ms/step - loss: 2.7021e-04\n","Epoch 3/25\n","966/966 [==============================] - 3s 4ms/step - loss: 1.0503e-04\n","Epoch 4/25\n","966/966 [==============================] - 3s 3ms/step - loss: 9.6609e-05\n","Epoch 5/25\n","966/966 [==============================] - 4s 4ms/step - loss: 9.2460e-05\n","Epoch 6/25\n","966/966 [==============================] - 3s 3ms/step - loss: 8.8878e-05\n","Epoch 7/25\n","966/966 [==============================] - 4s 4ms/step - loss: 8.5571e-05\n","Epoch 8/25\n","966/966 [==============================] - 3s 3ms/step - loss: 8.1560e-05\n","Epoch 9/25\n","966/966 [==============================] - 3s 3ms/step - loss: 7.5343e-05\n","Epoch 10/25\n","966/966 [==============================] - 3s 3ms/step - loss: 7.1866e-05\n","Epoch 11/25\n","966/966 [==============================] - 3s 3ms/step - loss: 6.8207e-05\n","Epoch 12/25\n","966/966 [==============================] - 3s 3ms/step - loss: 6.5403e-05\n","Epoch 13/25\n","966/966 [==============================] - 3s 3ms/step - loss: 6.2927e-05\n","Epoch 14/25\n","966/966 [==============================] - 3s 3ms/step - loss: 6.1383e-05\n","Epoch 15/25\n","966/966 [==============================] - 3s 3ms/step - loss: 5.9655e-05\n","Epoch 16/25\n","966/966 [==============================] - 3s 3ms/step - loss: 5.7292e-05\n","Epoch 17/25\n","966/966 [==============================] - 3s 3ms/step - loss: 5.6536e-05\n","Epoch 18/25\n","966/966 [==============================] - 3s 3ms/step - loss: 5.4863e-05\n","Epoch 19/25\n","966/966 [==============================] - 3s 4ms/step - loss: 5.4384e-05\n","Epoch 20/25\n","966/966 [==============================] - 3s 3ms/step - loss: 5.3778e-05\n","Epoch 21/25\n","966/966 [==============================] - 3s 3ms/step - loss: 5.2475e-05\n","Epoch 22/25\n","966/966 [==============================] - 4s 5ms/step - loss: 5.1593e-05\n","Epoch 23/25\n","966/966 [==============================] - 3s 3ms/step - loss: 5.0977e-05\n","Epoch 24/25\n","966/966 [==============================] - 3s 3ms/step - loss: 5.1259e-05\n","Epoch 25/25\n","966/966 [==============================] - 3s 3ms/step - loss: 4.9611e-05\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f4d62ba1fd0>"]},"metadata":{},"execution_count":22}]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}