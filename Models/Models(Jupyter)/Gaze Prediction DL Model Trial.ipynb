{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HuAXjDGJPDkw","executionInfo":{"status":"ok","timestamp":1675709171293,"user_tz":300,"elapsed":27155,"user":{"displayName":"Roman Shrestha","userId":"09406828698077064304"}},"outputId":"5704a30a-0818-4323-e213-e35cdaa4684c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mediapipe\n","  Downloading mediapipe-0.9.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting flatbuffers>=2.0\n","  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n","Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.8/dist-packages (from mediapipe) (3.19.6)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from mediapipe) (3.2.2)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.8/dist-packages (from mediapipe) (4.6.0.66)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from mediapipe) (1.21.6)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.8/dist-packages (from mediapipe) (22.2.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from mediapipe) (1.4.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (1.4.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (3.0.9)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->mediapipe) (1.15.0)\n","Installing collected packages: flatbuffers, mediapipe\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 1.12\n","    Uninstalling flatbuffers-1.12:\n","      Successfully uninstalled flatbuffers-1.12\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.9.2 requires flatbuffers<2,>=1.12, but you have flatbuffers 23.1.21 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed flatbuffers-23.1.21 mediapipe-0.9.1.0\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install mediapipe\n","import mediapipe as mp\n","import os\n","import json"]},{"cell_type":"markdown","source":["\n","\n","# Load Trained VAE Model"],"metadata":{"id":"qT7A8KJyPdNX"}},{"cell_type":"code","source":["vae_path = '/content/drive/Shareddrives/URSI 2022/Eye Tracking ML/vae_encoder/vae_2022-07-21_17:15:34'\n","vae_encoder = tf.keras.models.load_model(vae_path)\n","\n","vae_encoder.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":358},"id":"sXwLgQS4Pkbq","executionInfo":{"status":"error","timestamp":1675709173760,"user_tz":300,"elapsed":7,"user":{"displayName":"Roman Shrestha","userId":"09406828698077064304"}},"outputId":"f2770852-c8ba-4cf1-fbf0-b6c6502a9643"},"execution_count":null,"outputs":[{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-99e3c01dde12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvae_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/Shareddrives/URSI 2022/Eye Tracking ML/vae_encoder/vae_2022-07-21_17:15:34'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvae_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvae_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'No file or directory found at {filepath_str}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: No file or directory found at /content/drive/Shareddrives/URSI 2022/Eye Tracking ML/vae_encoder/vae_2022-07-21_17:15:34"]}]},{"cell_type":"markdown","source":["# Load MediaPipe model to get the set of mesh points"],"metadata":{"id":"EgXaAHvTPrEg"}},{"cell_type":"code","source":["mp_face_mesh = mp.solutions.face_mesh\n","\n","left_eye_point = set(sum(mp_face_mesh.FACEMESH_LEFT_EYE, ()))\n","right_eye_point = set(sum(mp_face_mesh.FACEMESH_RIGHT_EYE, ()))\n","left_iris_point = set(sum(mp_face_mesh.FACEMESH_LEFT_IRIS, ()))\n","right_iris_point = set(sum(mp_face_mesh.FACEMESH_RIGHT_IRIS, ()))\n","\n","face_oval_point = set(sum(mp_face_mesh.FACEMESH_FACE_OVAL, ()))\n","\n","#keypoints = left_eye_point.union(right_eye_point).union(left_iris_point).union(right_iris_point)\n","\n","keypoints = left_eye_point.union(right_eye_point).union(face_oval_point)\n","keypoints = keypoints.union([151, 9, 8, 168, 6, 197, 195, 5, 4])\n","\n","keypoints = sorted(list(keypoints))\n"],"metadata":{"id":"QnZt2oFiPtaq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Helper functions for the processing function\n"],"metadata":{"id":"MeWgu09-4nGa"}},{"cell_type":"code","source":["import statistics\n","import math\n","import random\n","\n","# Order in which calibration data is to be sorted\n","calibration_pts = [['10', '50'], ['10', '10'], ['90', '10'], ['50', '90'],\n","                   ['30', '70'], ['50', '50'], ['50', '10'], ['90', '90'],\n","                   ['70', '70'], ['70', '30'], ['10', '90'], ['90', '50'],\n","                   ['30', '30']]\n","\n","# Define indices of corresponding features in the 478 features list\n","irises = [469, 470, 471, 472, 474, 475, 476, 477]\n","face_cross = [226, 446, 9, 195]\n","\n","# Return coefficients a, b that represent the straight line \n","# constructed by the given points pt1, pt2 (y = ax + b)\n","def get_line(pt1, pt2):\n","  x1, y1 = pt1\n","  x2, y2 = pt2\n","  a = (y2 - y1) / (x2 - x1)\n","  b = y1 - (a * x1)\n","  return [a, b]\n","\n","# Return the coordinate of intersection of two straight lines\n","# l1, l2 in terms of [x, y]\n","def get_intersection(l1, l2):\n","  a1, b1 = l1\n","  a2, b2 = l2\n","  x = (b2 - b1) / (a1 - a2)\n","  y = a1 * x + b1\n","  return [x, y]\n","\n","# Return the angle that a vector needs to rotate counter-clockwisely\n","# in order to point at the same direction as the x-axis\n","def get_ccw_angle(vector):\n","  x, y = vector\n","  tan = y / x\n","  r = math.atan(tan)\n","  if x >= 0 and y > 0:\n","    pass\n","  elif x < 0 and y >= 0:\n","    r = r + math.pi\n","  elif x <= 0 and y < 0:\n","    r = r + math.pi\n","  elif x > 0 and y <= 0:\n","    r = r + 2 * math.pi\n","  else:\n","    r = 0\n","  return r\n","\n","# Given a list of 4 landmarks on the face mesh, construct a new coordinate\n","# system relative to the face, where the first two points from the list\n","# determine the x-axis and its intersection with the line constructed by\n","# the last two points is the origin of the new coordinate system. Return\n","# origin, rad. rad represents the angle the x-axis of the new coordinate\n","# system needs to rotate counter-clockwisely in order to point at the same\n","# direction as the x-axis of the coordinate system of the entire screen.\n","# The 2 return values serve to calculate the normalized iris features\n","def get_face_plane(points3d):\n","  points2d = []\n","  for point3d in points3d:\n","    x, y, z = point3d\n","    points2d.append([x, y])\n","  pt1, pt2, pt3, pt4 = points2d\n","  xaxis = get_line(pt1, pt2)\n","  yaxis = get_line(pt3, pt4)\n","  origin = get_intersection(xaxis, yaxis)\n","  v = []\n","  for a, b in zip(pt1, pt2):\n","    v.append(b - a)\n","  rad = 2 * math.pi - get_ccw_angle(v)\n","  return origin, rad\n","\n","# Return a set of normalized iris features relative to the face coordinate\n","# system given the original iris features, origin and the counter-clockwise\n","# angle of the face coordinate system relative to the entire screen\n","def rotate(origin, points, angle):\n","  ox, oy = origin\n","  normalized_points = []\n","  for point in points:\n","    px, py, pz = point\n","    qx = ox + math.cos(angle) * (px - ox) - math.sin(angle) * (py - oy)\n","    qy = oy + math.sin(angle) * (px - ox) + math.cos(angle) * (py - oy)\n","    nx = qx - ox\n","    ny = qy - oy\n","    normalized_points.append([nx, ny])\n","  return normalized_points\n","\n","# Return the 6-dimensional face representation and the normalized iris features\n","# given a list of subject videos\n","def predict_and_normalize(videos):\n","  face_frames = []\n","  normalized_irises = []\n","  for video in videos:\n","    frames = video[\"features\"]\n","    for frame in frames:\n","      face_frame = [frame[i] for i in keypoints]\n","      face_frames.append(face_frame)\n","      irises_data = [frame[i] for i in irises]\n","      o, r = get_face_plane([frame[i] for i in face_cross])\n","      normalized_data = rotate(o, irises_data, r)\n","      normalized_irises.append(normalized_data)\n","  # The latent features are eventually converted because vae_encoder.predict()\n","  # only supports a 3D tensor as the input\n","  latent_features = vae_encoder.predict(face_frames)\n","  latent_features = latent_features.tolist()\n","  return latent_features, normalized_irises\n","\n","# Return a list of videos with the given 'block' attribute\n","def get_block_data(block_num, subject_data):\n","  block_data = []\n","  for video in subject_data:\n","    if video['block'] == block_num:\n","      block_data.append(video)\n","  return block_data\n","\n","# Return 2 lists of videos for calibration and test, respectively,\n","# given a list of videos share the same 'block' attribute\n","def get_ct_data(vlst):\n","  calibration_data = []\n","  test_data = []\n","  for video in vlst:\n","    if video['phase'] == 'calibration':\n","      calibration_data.append(video)\n","    else:\n","      test_data.append(video)\n","  return calibration_data, test_data\n","\n","# Sort calibration data respective to the reference order\n","def sort_calibration(c_data):\n","  sorted_data = []\n","  for pt in calibration_pts:\n","    x = pt[0]\n","    y = pt[1]\n","    for video in c_data:\n","      if video['x'] == x and video['y'] == y:\n","        sorted_data.append(video)\n","  return sorted_data"],"metadata":{"id":"PN791R-m4tm4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# The main processing function"],"metadata":{"id":"H5ekr3BsUY_p"}},{"cell_type":"code","source":["# This giant function takes the json file of one individual subject as input\n","# and processes it. Eventually, it returns a list of inputs and its \n","# corresponding list of target gaze points.\n","\n","def get_inputs_targets(subject_name, subject_data):\n","\n","  print('Begin to process subject ' + subject_name + '...')\n","\n","  latent_features, normalized_irises = predict_and_normalize(subject_data)\n","\n","  # latent_features (3605 x 6) and normalized_irises (3605 x 8 x 2) are two \n","  # separate lists. We may want to merge them together for convenience so that\n","  # every element in aggregate_features contains all crucial information of the\n","  # face in one frame\n","\n","  aggregate_features = []\n","\n","  for a, b in zip(latent_features, normalized_irises):\n","    aggregate_features.append([a, b])\n","\n","  print('Face representation and normalized iris features sorted...')\n","\n","  # We want to make a copy of subject_data and replace the 'features' content\n","  # of every video with information of the latent features and normalized irises.\n","  # We want to put these info back to the dictionary because eventually we need\n","  # to sort the input for the incoming deep learning model according to the \n","  # 'phase' and the 'block' attributes\n","\n","  # This counter records number of frames processed.\n","  # It updates per video processed\n","  frames_counter = 0\n","\n","  subject_data_copy = subject_data.copy()\n","\n","  print('Made a copy of the subject ' + subject_name + '...')\n","\n","  # Loop through videos\n","  for video in subject_data_copy:\n","    # Check number of frames of the video\n","    frames_num = len(video['features'])\n","    # Index of the first element we want from aggregate_features\n","    head = frames_counter\n","    # Index of the first element we want from aggregate_features FOR THE NEXT VIDEO\n","    tail = head + frames_num\n","    # Rewrite the 'features' attribute\n","    video['features'] = [aggregate_features[i] for i in range(head, tail)]\n","    # Update counter\n","    frames_counter = tail\n","\n","  print('\\\"features\\\" attribute rewritten...')\n","\n","  # Inputs list for the deep learning model\n","  inputs = []\n","  # Targets list corresponding to the inputs list\n","  targets = []\n","\n","  # Loop through blocks\n","  for i in ['0', '1', '2']:\n","    block_data = get_block_data(i, subject_data_copy)\n","    c_data, t_data = get_ct_data(block_data)\n","    # Check if there are 13 calibration videos for this block\n","    if len(c_data) < 13:\n","      # If not, print error messages and return\n","      print('ERROR MESSAGE: ' + subject_name + ' has insufficient number of calibration data in block ' + i + ': ' + str(len(c_data)))\n","      return False\n","    c_data = sort_calibration(c_data)\n","    c_v_frames = []\n","    for c in c_data:\n","      c_v_frames.append(len(c['features']))\n","    # Loop through test videos first. The targets depend on the test videos NOT the calibration videos\n","    for t_video in t_data:\n","      # Note down the target gaze coordinate\n","      target = [int(t_video['x']), int(t_video['y'])]\n","      # Declare an individual input list. There should be eventually 14 elements in it: 1 test video, all 13 calibration videos\n","      input = []\n","      t_frames = t_video['features']\n","      try:\n","        # Try random frame selection\n","        input.append(random.choice(t_frames))\n","      except:\n","        # Catch the exception and go to the next test video\n","        x, y = target\n","        print('ERROR MESSAGE: ' + subject_name + ' has a test video with 0 frames')\n","        print('-----> block ' + i + ', (' + x + ', ' + y + ')')\n","        continue\n","      # For every test video, append all calibration videos info\n","      c_idx = 0\n","      for c_video in c_data:\n","        c_frames = c_video['features']\n","        try:\n","          # Try random frame selection\n","          input.append(random.choice(c_frames))\n","          c_idx += 1\n","        except:\n","          # Catch the exception and stop processing the entire subject\n","          x, y = calibration_pts[c_idx]\n","          print('ERROR MESSAGE: ' + subject_name + ' has a calibration video with 0 frames')\n","          print('-----> block ' + i + ', (' + x + ', ' + y + ')')\n","          return False\n","      inputs.append(input)\n","      targets.append(target)\n","\n","  return inputs, targets"],"metadata":{"id":"I3w9ENeWUcWf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load subject json files and export inputs json files"],"metadata":{"id":"0_XQaZjluL_6"}},{"cell_type":"code","source":["# At first load all subject json files from the json folder and\n","# store them into a giant dictionary called json_data\n","\n","json_path = '/content/drive/Shareddrives/URSI 2022/Eye Tracking ML/prolific_eye_data_experiment/json/'\n","all_json_files = os.listdir(json_path)\n","\n","json_data = {}\n","i = 0\n","\n","for filename in all_json_files:\n","    # *** TEMPORARY SOLUTION ***\n","    # Control amount of json files stored in json_data to avoid overloading RAM\n","    # Change conditions to get different set of files (limit appears to be ~30)\n","    if (i >= 120 and i <= 148): \n","        with open(json_path + filename, 'r') as file:\n","            s_data = json.load(file)\n","            json_data = {**json_data, **s_data}\n","    i += 1\n","for k, v in json_data.items():\n","  print(k)"],"metadata":{"id":"wGLyNMhzW7pT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659023784531,"user_tz":240,"elapsed":4352,"user":{"displayName":"Oliravan Eswaramoorthy","userId":"13611592422418692717"}},"outputId":"0ac986c8-11ba-422f-d96d-3b1a6e43c22e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["o1cekcwj\n"]}]},{"cell_type":"code","source":["# Loop through every subject info in json_data and store inputs info of\n","# every subject as a dictionary, eventually export it in the json format\n","# to the folder called json_inputs\n","\n","for k, v in json_data.items():\n","  try:\n","    i, t = get_inputs_targets(k, v)\n","    subject_json = {\n","        k: {\n","            'x': i,\n","            'y': t\n","        }\n","    }\n","    with open('/content/drive/Shareddrives/URSI 2022/Eye Tracking ML/json_inputs/'+k+'_inputs.json', 'w') as file:\n","      json.dump(subject_json, file)\n","      print('Subject ' + k + ' data has been successfully exported!\\n')\n","  except:\n","    print('NOTICE: Subject ' + k + ' is abandoned\\n')\n","    continue"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RLWqjuA02yLi","executionInfo":{"status":"ok","timestamp":1659023504690,"user_tz":240,"elapsed":143301,"user":{"displayName":"Oliravan Eswaramoorthy","userId":"13611592422418692717"}},"outputId":"fe3dde73-0053-4254-f825-dd0c54cc2ab6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Begin to process subject 9u78fe8u...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject 9u78fe8u...\n","\"features\" attribute rewritten...\n","Subject 9u78fe8u data has been successfully exported!\n","\n","Begin to process subject 4v1so2k2...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject 4v1so2k2...\n","\"features\" attribute rewritten...\n","Subject 4v1so2k2 data has been successfully exported!\n","\n","Begin to process subject ln1addz0...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject ln1addz0...\n","\"features\" attribute rewritten...\n","Subject ln1addz0 data has been successfully exported!\n","\n","Begin to process subject 4h62dlxl...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject 4h62dlxl...\n","\"features\" attribute rewritten...\n","Subject 4h62dlxl data has been successfully exported!\n","\n","Begin to process subject zajruoul...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject zajruoul...\n","\"features\" attribute rewritten...\n","Subject zajruoul data has been successfully exported!\n","\n","Begin to process subject g2sf2265...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject g2sf2265...\n","\"features\" attribute rewritten...\n","Subject g2sf2265 data has been successfully exported!\n","\n","Begin to process subject cxdc2ff8...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject cxdc2ff8...\n","\"features\" attribute rewritten...\n","Subject cxdc2ff8 data has been successfully exported!\n","\n","Begin to process subject 8jnks6k4...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject 8jnks6k4...\n","\"features\" attribute rewritten...\n","Subject 8jnks6k4 data has been successfully exported!\n","\n","Begin to process subject 44shkkna...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject 44shkkna...\n","\"features\" attribute rewritten...\n","Subject 44shkkna data has been successfully exported!\n","\n","Begin to process subject m254uvjd...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject m254uvjd...\n","\"features\" attribute rewritten...\n","Subject m254uvjd data has been successfully exported!\n","\n","Begin to process subject u0qhupu2...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject u0qhupu2...\n","\"features\" attribute rewritten...\n","Subject u0qhupu2 data has been successfully exported!\n","\n","Begin to process subject h9s0pk7o...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject h9s0pk7o...\n","\"features\" attribute rewritten...\n","Subject h9s0pk7o data has been successfully exported!\n","\n","Begin to process subject owrk4hq7...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject owrk4hq7...\n","\"features\" attribute rewritten...\n","Subject owrk4hq7 data has been successfully exported!\n","\n","Begin to process subject hj0lwre6...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject hj0lwre6...\n","\"features\" attribute rewritten...\n","Subject hj0lwre6 data has been successfully exported!\n","\n","Begin to process subject bccxvpl3...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject bccxvpl3...\n","\"features\" attribute rewritten...\n","Subject bccxvpl3 data has been successfully exported!\n","\n","Begin to process subject u7xm3z3q...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject u7xm3z3q...\n","\"features\" attribute rewritten...\n","Subject u7xm3z3q data has been successfully exported!\n","\n","Begin to process subject nh8ncnow...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject nh8ncnow...\n","\"features\" attribute rewritten...\n","Subject nh8ncnow data has been successfully exported!\n","\n","Begin to process subject 1h1dsh7s...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject 1h1dsh7s...\n","\"features\" attribute rewritten...\n","Subject 1h1dsh7s data has been successfully exported!\n","\n","Begin to process subject 3swzt0jr...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject 3swzt0jr...\n","\"features\" attribute rewritten...\n","Subject 3swzt0jr data has been successfully exported!\n","\n","Begin to process subject 78beu8ch...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject 78beu8ch...\n","\"features\" attribute rewritten...\n","Subject 78beu8ch data has been successfully exported!\n","\n","Begin to process subject pa4f1gfc...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject pa4f1gfc...\n","\"features\" attribute rewritten...\n","Subject pa4f1gfc data has been successfully exported!\n","\n","Begin to process subject n4q2mn6b...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject n4q2mn6b...\n","\"features\" attribute rewritten...\n","Subject n4q2mn6b data has been successfully exported!\n","\n","Begin to process subject yscaxtlm...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject yscaxtlm...\n","\"features\" attribute rewritten...\n","Subject yscaxtlm data has been successfully exported!\n","\n","Begin to process subject 3641lwa9...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject 3641lwa9...\n","\"features\" attribute rewritten...\n","Subject 3641lwa9 data has been successfully exported!\n","\n","Begin to process subject r30go9o8...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject r30go9o8...\n","\"features\" attribute rewritten...\n","Subject r30go9o8 data has been successfully exported!\n","\n","Begin to process subject m92x5ume...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject m92x5ume...\n","\"features\" attribute rewritten...\n","Subject m92x5ume data has been successfully exported!\n","\n","Begin to process subject jxk6nav6...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject jxk6nav6...\n","\"features\" attribute rewritten...\n","Subject jxk6nav6 data has been successfully exported!\n","\n","Begin to process subject 19207bos...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject 19207bos...\n","\"features\" attribute rewritten...\n","Subject 19207bos data has been successfully exported!\n","\n","Begin to process subject o1cekcwj...\n","Face representation and normalized iris features sorted...\n","Made a copy of the subject o1cekcwj...\n","\"features\" attribute rewritten...\n","Subject o1cekcwj data has been successfully exported!\n","\n"]}]}]}